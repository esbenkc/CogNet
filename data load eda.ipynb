{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_msg_files(zip_path, target_dir):\n",
    "    with ZipFile(zip_path, 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        all_files = zipObj.namelist()\n",
    "        for file in all_files:\n",
    "            if file.endswith(\".json\"):\n",
    "                zipObj.extract(file, target_dir)\n",
    "\n",
    "                \n",
    "def yield_msg_files(zip_path):\n",
    "    \"\"\" Creates generator for files in zipdir \"\"\"\n",
    "    with ZipFile(zip_path, 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        all_files = zipObj.namelist()\n",
    "        for file in all_files:\n",
    "            with zipObj.open(file, \"r\") as myfile:\n",
    "                try:\n",
    "                    yield json.loads(json.load(myfile))\n",
    "                except TypeError:\n",
    "                    yield file\n",
    "\n",
    "\n",
    "def count_msg_files(zip_path):\n",
    "    \"\"\"Counts the number of conversations in zip-file\"\"\"\n",
    "    with ZipFile(zip_path, \"r\") as zipObj:\n",
    "        return len(zipObj.namelist())\n",
    "    \n",
    "\n",
    "def read_zip_file(zip_path, file_name):\n",
    "    with ZipFile(zip_path, \"r\") as zipObj:\n",
    "        with zipObj.open(file_name, \"r\") as f:\n",
    "            return json.load(f)\n",
    "                    \n",
    "def read_json(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def read_convo(file):\n",
    "    \"\"\"reads conversation json file to dict \"\"\"\n",
    "    return json.loads(read_json(file))\n",
    "\n",
    "def hash_name(name):\n",
    "    \"\"\" simplified version (no salt) \"\"\"\n",
    "    return hashlib.sha1(name.encode()).hexdigest()\n",
    "\n",
    "def create_group_id(groupchat):\n",
    "    \"\"\"creates a group id based on participant names\"\"\"\n",
    "    participant_string = \"\".join(sorted(groupchat[\"participants\"]))\n",
    "    return hash_name(participant_string)\n",
    "\n",
    "def find_most_common(participant_list):\n",
    "    \"\"\"finds most common element in list \"\"\"\n",
    "    return Counter(participant_list).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_reactions(msg, rel_list):\n",
    "    \"\"\" Appends reaction to a reaction list (preprocessing step) \"\"\"\n",
    "    if \"reactions\" in msg.keys():\n",
    "        for reaction in msg[\"reactions\"]:\n",
    "            reaction_dict = {\"from\": reaction, \n",
    "                             \"to\": msg[\"sender_name\"], \n",
    "                             \"timestamp\": msg[\"timestamp_ms\"], \n",
    "                             \"rel_type\": \"reaction\"}\n",
    "            rel_list.append(reaction_dict)\n",
    "\n",
    "            \n",
    "            \n",
    "def create_member_edges(group_convo, group_id):\n",
    "    \"\"\" \n",
    "    Create participant --> group relations for a conversation \n",
    "    NB: These will have timestamp as nan!\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\"from\": group_convo[\"participants\"], \n",
    "                          \"to\": group_id, \n",
    "                          \"timestamp\": np.nan, \n",
    "                          \"rel_type\": \"group\"})\n",
    "\n",
    "def process_group_messages(group_convo, group_id):\n",
    "    \"\"\" Create a nice dataframe with all the messages from group chat\"\"\"\n",
    "    assert group_convo[\"thread_type\"] == \"RegularGroup\"\n",
    "    group_msgs = pd.DataFrame(index=range(len(group_convo[\"messages\"])), \n",
    "                              columns=[\"from\", \"to\", \"timestamp\", \"rel_type\"])\n",
    "    group_msgs = group_msgs.assign(to = group_id, rel_type = \"msg\")\n",
    "    rel_list = []\n",
    "    for i, msg in enumerate(group_convo[\"messages\"]):\n",
    "        group_msgs.loc[i, \"from\"] = msg[\"sender_name\"]\n",
    "        group_msgs.loc[i, \"timestamp\"] = msg[\"timestamp_ms\"]\n",
    "        add_reactions(msg, rel_list)\n",
    "    return pd.concat([group_msgs, pd.DataFrame(rel_list)])\n",
    "\n",
    "def process_group_edges(group_convo):\n",
    "    \"\"\" Full pipeline for processing group chats \"\"\"\n",
    "    group_id = create_group_id(group_convo)\n",
    "    group_msgs = process_group_messages(group_convo, group_id)\n",
    "    group_members = create_member_edges(group_convo, group_id)\n",
    "    return pd.concat([group_msgs, group_members]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def process_msgs(convo):\n",
    "    \"\"\" Processes messages and returns a nice dataframe :)) \"\"\"\n",
    "    if len(convo[\"participants\"]) == 1:\n",
    "        return None\n",
    "    assert convo[\"thread_type\"] == \"Regular\"\n",
    "    msgs = pd.DataFrame(index=range(len(convo[\"messages\"])), \n",
    "                        columns=[\"from\", \"to\", \"timestamp\", \"rel_type\"])\n",
    "    msgs = msgs.assign(rel_type = \"msg\")\n",
    "    rel_list = []\n",
    "    for i, msg in enumerate(convo[\"messages\"]):\n",
    "        if \"call_duration\" in msg.keys():\n",
    "            continue\n",
    "        msgs.loc[i, \"from\"] = msg[\"sender_name\"]\n",
    "        msgs.loc[i, \"to\"] = msg[\"receiver_name\"]\n",
    "        msgs.loc[i, \"timestamp\"] = msg[\"timestamp_ms\"]\n",
    "        add_reactions(msg, rel_list)\n",
    "    return pd.concat([msgs.dropna(subset=[\"from\"])\n",
    "                            , pd.DataFrame(rel_list)])\n",
    "\n",
    "\n",
    "def fix_dropout_dict(data_path):    \n",
    "    \"\"\"adds name to dropout dict as well as fixes key\"\"\"\n",
    "    participant_list = []\n",
    "    num_two_person = 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        for convo in yield_msg_files(data_path):\n",
    "            is_two_person = convo[\"thread_type\"] == \"Regular\"\n",
    "            if is_two_person:\n",
    "                num_two_person += 1\n",
    "                participant_list.extend(convo[\"participants\"])      \n",
    "            if num_two_person == 2:\n",
    "                stop = True\n",
    "                break\n",
    "        \n",
    "    dropout_dict = read_zip_file(data_path, \"dropout.json\")\n",
    "    dropout_dict[\"still_cogsci\"] = dropout_dict.pop(\"is_dropout\")\n",
    "    dropout_dict[\"name\"] = find_most_common(participant_list)\n",
    "    return dropout_dict\n",
    "\n",
    "\n",
    "def process_person(data_path):\n",
    "    \"\"\"\n",
    "    processes all conversations from one person \n",
    "    (inputs path to zip-file)\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    for convo in yield_msg_files(data_path):\n",
    "        if type(convo) == str:\n",
    "            continue\n",
    "        elif convo[\"thread_type\"] == \"Regular\":\n",
    "            df_list.append(process_msgs(convo))\n",
    "        elif convo[\"thread_type\"] == \"RegularGroup\":\n",
    "            df_list.append(process_group_edges(convo))\n",
    "        else:\n",
    "            print(convo[\"thread_type\"])\n",
    "    try:\n",
    "        return pd.concat(df_list)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_dropout_df(data_paths):\n",
    "    \"\"\"Full pipeline for creating df with from the dropout.json \"\"\"\n",
    "    dropout_list = [None for _ in range(len(data_paths))]\n",
    "    for i, data_path in enumerate(data_paths):\n",
    "        dropout_list[i] = fix_dropout_dict(data_path)\n",
    "    return pd.DataFrame(dropout_list)\n",
    "\n",
    "def anonymize_filename(data_path):\n",
    "    \"\"\" Removes the actual name from the filename (weird google thing)\"\"\"\n",
    "    problem_part = data_path.name.find(\" -\")\n",
    "    new_name = data_path.parent / Path(data_path.name[:problem_part] + \".zip\")\n",
    "    data_path.rename(new_name)\n",
    "    \n",
    "def anonymize_folder(data_folder):\n",
    "    \"\"\"Anonymizes all filenames in folder (from google thing)\"\"\"\n",
    "    problem_paths = data_folder.glob(\"*-*.zip\")\n",
    "    for file in problem_paths:\n",
    "        anonymize_filename(file)\n",
    "        \n",
    "def find_unique_ids(dropout_df, full_df):\n",
    "    \"\"\" Returns unique ids (groups and people) for filtering\"\"\"\n",
    "    unique_people = dropout_df[\"name\"].unique()\n",
    "    unique_groups = unique_master.loc[unique_master[\"rel_type\"] == \"group\", \"to\"].unique()\n",
    "    return set(np.concatenate((unique_people, unique_groups)))\n",
    "\n",
    "\n",
    "def filter_consent(full_df):\n",
    "    unique_ids = find_unique_ids(dropout_df, full_df)\n",
    "    consenting_filter = full_df[\"from\"].isin(unique_ids) & full_df[\"to\"].isin(unique_ids)\n",
    "    return full_df[consenting_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./data\")\n",
    "anonymize_folder(DATA_DIR)\n",
    "data_paths = list(DATA_DIR.glob(\"*.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_df = create_dropout_df(data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [None for _ in range(len(data_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing person 1 out of 29...\n",
      "Pending\n",
      "processing person 2 out of 29...\n",
      "processing person 3 out of 29...\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "Pending\n",
      "processing person 4 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "processing person 5 out of 29...\n",
      "processing person 6 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "processing person 7 out of 29...\n",
      "Pending\n",
      "PendingGroup\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "processing person 8 out of 29...\n",
      "processing person 9 out of 29...\n",
      "processing person 10 out of 29...\n",
      "processing person 11 out of 29...\n",
      "PendingGroup\n",
      "processing person 12 out of 29...\n",
      "PendingGroup\n",
      "processing person 13 out of 29...\n",
      "Pending\n",
      "processing person 14 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "processing person 15 out of 29...\n",
      "PendingGroup\n",
      "Pending\n",
      "processing person 16 out of 29...\n",
      "Pending\n",
      "processing person 17 out of 29...\n",
      "processing person 18 out of 29...\n",
      "Pending\n",
      "processing person 19 out of 29...\n",
      "processing person 20 out of 29...\n",
      "processing person 21 out of 29...\n",
      "PendingGroup\n",
      "Pending\n",
      "processing person 22 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "processing person 23 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "processing person 24 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "processing person 25 out of 29...\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "processing person 26 out of 29...\n",
      "Pending\n",
      "processing person 27 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "processing person 28 out of 29...\n",
      "processing person 29 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "all done!\n"
     ]
    }
   ],
   "source": [
    "for i, data_path in enumerate(data_paths):\n",
    "    if data_list[i] is None:\n",
    "        print(f\"processing person {i+1} out of {len(data_paths)}...\")\n",
    "        try:\n",
    "            data_list[i] = process_person(data_path)\n",
    "        except FileNotFoundError:\n",
    "            print(\"no file here\")\n",
    "            continue\n",
    "print(\"all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_master = master_df.drop_duplicates(subset=['timestamp'])\n",
    "consent_df = filter_consent(unique_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "consent_df.to_csv(\"all_messages.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}