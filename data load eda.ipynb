{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib, uuid\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 18,
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_name(name):\n",
    "    \"\"\" simplified version (no salt) \"\"\"\n",
    "    return hashlib.sha1(name.encode()).hexdigest()\n",
    "\n",
    "def hash_with_salt(s):\n",
    "    \"\"\" Hashes a string with a randomly generated salt \"\"\"\n",
    "    salt = uuid.uuid4().hex\n",
    "    return hashlib.sha512(s + salt).hexdigest()\n",
    "\n",
    "\n",
    "def check_name(hashed_name, new_name):\n",
    "    return hashed_name == hash_name(new_name)\n",
    "\n",
    "\n",
    "def is_a_cogsci(cogsci_hashes, new_name):\n",
    "    return any(check_name(hsh, new_name) for hsh in cogsci_hashes)\n",
    "\n",
    "\n",
    "def unzip_msg_files(zip_path, target_dir):\n",
    "    with ZipFile(zip_path, 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        all_files = zipObj.namelist()\n",
    "        for file in all_files:\n",
    "            if file.endswith(\".json\"):\n",
    "                zipObj.extract(file, target_dir)\n",
    "\n",
    "                \n",
    "def yield_msg_files(zip_path):\n",
    "    \"\"\" Creates generator for files in zipdir \"\"\"\n",
    "    with ZipFile(zip_path, 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        all_files = zipObj.namelist()\n",
    "        for file in all_files:\n",
    "            with zipObj.open(file, \"r\") as myfile:\n",
    "                try:\n",
    "                    yield json.loads(json.load(myfile))\n",
    "                except TypeError:\n",
    "                    yield file\n",
    "\n",
    "\n",
    "def count_msg_files(zip_path):\n",
    "    \"\"\"Counts the number of conversations in zip-file\"\"\"\n",
    "    with ZipFile(zip_path, \"r\") as zipObj:\n",
    "        return len(zipObj.namelist())\n",
    "    \n",
    "\n",
    "def read_zip_file(zip_path, file_name):\n",
    "    with ZipFile(zip_path, \"r\") as zipObj:\n",
    "        with zipObj.open(file_name, \"r\") as f:\n",
    "            return json.load(f)\n",
    "                    \n",
    "def read_json(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def read_convo(file):\n",
    "    \"\"\"reads conversation json file to dict \"\"\"\n",
    "    return json.loads(read_json(file))\n",
    "\n",
    "def hash_name(name):\n",
    "    \"\"\" simplified version (no salt) \"\"\"\n",
    "    return hashlib.sha1(name.encode()).hexdigest()\n",
    "\n",
    "def create_group_id(groupchat):\n",
    "    \"\"\"creates a group id based on participant names\"\"\"\n",
    "    participant_string = \"\".join(sorted(groupchat[\"participants\"]))\n",
    "    return hash_name(participant_string)\n",
    "\n",
    "def find_most_common(participant_list):\n",
    "    \"\"\"finds most common element in list \"\"\"\n",
    "    return Counter(participant_list).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def add_reactions(msg, rel_list):\n",
    "    \"\"\" Appends reaction to a reaction list (preprocessing step) \"\"\"\n",
    "    if \"reactions\" in msg.keys():\n",
    "        for reaction in msg[\"reactions\"]:\n",
    "            reaction_dict = {\"from\": reaction, \n",
    "                             \"to\": msg[\"sender_name\"], \n",
    "                             \"timestamp\": msg[\"timestamp_ms\"], \n",
    "                             \"rel_type\": \"reaction\"}\n",
    "            rel_list.append(reaction_dict)\n",
    "\n",
    "            \n",
    "            \n",
    "def create_member_edges(group_convo, group_id):\n",
    "    \"\"\" \n",
    "    Create participant --> group relations for a conversation \n",
    "    NB: These will have timestamp as nan!\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\"from\": group_convo[\"participants\"], \n",
    "                          \"to\": group_id, \n",
    "                          \"timestamp\": np.nan, \n",
    "                          \"rel_type\": \"group\"})\n",
    "\n",
    "def process_group_messages(group_convo, group_id):\n",
    "    \"\"\" Create a nice dataframe with all the messages from group chat\"\"\"\n",
    "    assert group_convo[\"thread_type\"] == \"RegularGroup\"\n",
    "    group_msgs = pd.DataFrame(index=range(len(group_convo[\"messages\"])), \n",
    "                              columns=[\"from\", \"to\", \"timestamp\", \"rel_type\"])\n",
    "    group_msgs = group_msgs.assign(to = group_id, rel_type = \"msg\")\n",
    "    rel_list = []\n",
    "    for i, msg in enumerate(group_convo[\"messages\"]):\n",
    "        group_msgs.loc[i, \"from\"] = msg[\"sender_name\"]\n",
    "        group_msgs.loc[i, \"timestamp\"] = msg[\"timestamp_ms\"]\n",
    "        add_reactions(msg, rel_list)\n",
    "    return pd.concat([group_msgs, pd.DataFrame(rel_list)])\n",
    "\n",
    "def process_group_edges(group_convo):\n",
    "    \"\"\" Full pipeline for processing group chats \"\"\"\n",
    "    group_id = create_group_id(group_convo)\n",
    "    group_msgs = process_group_messages(group_convo, group_id)\n",
    "    group_members = create_member_edges(group_convo, group_id)\n",
    "    return pd.concat([group_msgs, group_members]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def process_msgs(convo):\n",
    "    \"\"\" Processes messages and returns a nice dataframe :)) \"\"\"\n",
    "    if len(convo[\"participants\"]) == 1:\n",
    "        return None\n",
    "    assert convo[\"thread_type\"] == \"Regular\"\n",
    "    msgs = pd.DataFrame(index=range(len(convo[\"messages\"])), \n",
    "                        columns=[\"from\", \"to\", \"timestamp\", \"rel_type\"])\n",
    "    msgs = msgs.assign(rel_type = \"msg\")\n",
    "    rel_list = []\n",
    "    for i, msg in enumerate(convo[\"messages\"]):\n",
    "        if \"call_duration\" in msg.keys():\n",
    "            continue\n",
    "        msgs.loc[i, \"from\"] = msg[\"sender_name\"]\n",
    "        msgs.loc[i, \"to\"] = msg[\"receiver_name\"]\n",
    "        msgs.loc[i, \"timestamp\"] = msg[\"timestamp_ms\"]\n",
    "        add_reactions(msg, rel_list)\n",
    "    return pd.concat([msgs.dropna(subset=[\"from\"])\n",
    "                            , pd.DataFrame(rel_list)])\n",
    "\n",
    "\n",
    "def fix_dropout_dict(data_path):    \n",
    "    \"\"\"adds name to dropout dict as well as fixes key\"\"\"\n",
    "    participant_list = []\n",
    "    num_two_person = 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        for convo in yield_msg_files(data_path):\n",
    "            is_two_person = convo[\"thread_type\"] == \"Regular\"\n",
    "            if is_two_person:\n",
    "                num_two_person += 1\n",
    "                participant_list.extend(convo[\"participants\"])      \n",
    "            if num_two_person == 2:\n",
    "                stop = True\n",
    "                break\n",
    "        \n",
    "    dropout_dict = read_zip_file(data_path, \"dropout.json\")\n",
    "    dropout_dict[\"still_cogsci\"] = dropout_dict.pop(\"is_dropout\")\n",
    "    dropout_dict[\"name\"] = find_most_common(participant_list)\n",
    "    return dropout_dict\n",
    "\n",
    "\n",
    "def process_person(data_path):\n",
    "    \"\"\"\n",
    "    processes all conversations from one person \n",
    "    (inputs path to zip-file)\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    for convo in yield_msg_files(data_path):\n",
    "        if type(convo) == str:\n",
    "            continue\n",
    "        elif convo[\"thread_type\"] == \"Regular\":\n",
    "            df_list.append(process_msgs(convo))\n",
    "        elif convo[\"thread_type\"] == \"RegularGroup\":\n",
    "            df_list.append(process_group_edges(convo))\n",
    "        else:\n",
    "            print(convo[\"thread_type\"])\n",
    "    try:\n",
    "        return pd.concat(df_list)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_dropout_df(data_paths):\n",
    "    \"\"\"Full pipeline for creating df with from the dropout.json \"\"\"\n",
    "    dropout_list = [None for _ in range(len(data_paths))]\n",
    "    for i, data_path in enumerate(data_paths):\n",
    "        dropout_list[i] = fix_dropout_dict(data_path)\n",
    "    return pd.DataFrame(dropout_list)\n",
    "\n",
    "def anonymize_filename(data_path):\n",
    "    \"\"\" Removes the actual name from the filename (weird google thing)\"\"\"\n",
    "    problem_part = data_path.name.find(\" -\")\n",
    "    new_name = data_path.parent / Path(data_path.name[:problem_part] + \".zip\")\n",
    "    data_path.rename(new_name)\n",
    "    \n",
    "def anonymize_folder(data_folder):\n",
    "    \"\"\"Anonymizes all filenames in folder (from google thing)\"\"\"\n",
    "    problem_paths = data_folder.glob(\"*-*.zip\")\n",
    "    for file in problem_paths:\n",
    "        anonymize_filename(file)\n",
    "        \n",
    "def find_unique_ids(cogscis, full_df):\n",
    "    \"\"\" Returns unique ids (groups and people) for filtering\"\"\"\n",
    "    unique_people = pd.Series(cogscis).unique()\n",
    "    unique_groups = unique_master.loc[unique_master[\"rel_type\"] == \"group\", \"to\"].unique()\n",
    "    return set(np.concatenate((unique_people, unique_groups)))\n",
    "\n",
    "\n",
    "def load_cog_hash():\n",
    "    \"\"\"Loads the pickled cogsci hash file (should be in parent dir)\"\"\"\n",
    "    with open(Path(\"../cogsci19_2.pkl\"), \"rb\") as cog:\n",
    "        return pickle.load(cog)\n",
    "    \n",
    "    \n",
    "def find_non_cogs(full_df):\n",
    "    \"\"\"Finds ids not recognized by the cogsci hash \"\"\"\n",
    "    cogs = load_cog_hash()\n",
    "    non_cog_df = full_df[~full_df[\"from\"].isin(cogs)]\n",
    "    non_cog_groups = non_cog_df.loc[non_cog_df[\"rel_type\"] == \"group\", \"to\"].unique()\n",
    "    return set(np.concatenate((non_cog_df[\"from\"], non_cog_groups)))\n",
    "\n",
    "\n",
    "def remove_non_cogs(full_df):\n",
    "    \"\"\" Remove all people not recognized by their cogsci hash\"\"\"\n",
    "    all_non_cogs = find_non_cogs(full_df)\n",
    "    non_cog_filter = ~(unique_master[\"to\"].isin(all_non_cogs) | unique_master[\"from\"].isin(all_non_cogs))\n",
    "    return full_df[non_cog_filter]\n",
    "\n",
    "\n",
    "def filter_consent(full_df, dropout_df):\n",
    "    \"\"\" Filters so only people we have data / consent from \"\"\"\n",
    "    unique_ids = find_unique_ids(dropout_df, full_df)\n",
    "    consenting_filter = full_df[\"from\"].isin(unique_ids) & full_df[\"to\"].isin(unique_ids)\n",
    "    return full_df[consenting_filter]\n",
    "\n",
    "def calc_group_sizes(df):\n",
    "    \"\"\" Finds the size of each groupchat in the df \"\"\"\n",
    "    return df[df[\"rel_type\"] == \"group\"].groupby(\"to\")[\"from\"].agg(\"count\")\n",
    "\n",
    "\n",
    "def find_hex(s):\n",
    "    \"\"\" Checks whether names are valid hashes \"\"\"\n",
    "    try:\n",
    "        int(s, 16)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def add_hash_check(df):\n",
    "    \"\"\" Adds columns to the df checking which names are hashed \"\"\"\n",
    "    return df.assign(from_hex = df[\"from\"].apply(lambda x: find_hex(x)),\n",
    "                     to_hex = df[\"to\"].apply(lambda x: find_hex(x)))\n",
    "\n",
    "def hash_plaintext(df):\n",
    "    \"\"\" hash names that haven't been hashed\"\"\"\n",
    "    df.loc[~df[\"from_hex\"], \"from\"] = df.loc[~df[\"from_hex\"], \"from\"].apply(lambda x: hash_name(x))\n",
    "    df.loc[~df[\"to_hex\"], \"to\"] = df.loc[~df[\"to_hex\"], \"to\"].apply(lambda x: hash_name(x))\n",
    "    \n",
    "\n",
    "def fix_vero(df):\n",
    "    \"\"\" Fixes weird vero bug \"\"\"\n",
    "    df.loc[df[\"from\"] == \"Verus Juhasz\", \"from\"] = hash_name(\"Verus Juhasz\")\n",
    "    df.loc[df[\"to\"] == \"Verus Juhasz\", \"to\"] = hash_name(\"Verus Juhasz\")\n",
    "    \n",
    "def calculate_group_weights(group_sizes):\n",
    "    \"\"\" add weights to the convo depending on size \"\"\"\n",
    "    return np.floor(np.log2(group_sizes.max()) * (1 / np.log2(group_sizes)))\n",
    "\n",
    "def create_random_ids(id_tuple):\n",
    "    \"\"\"Inputs a tuple of pd.Series with ids and outputs a dict mapping them to new randomly generated ids\"\"\"\n",
    "    unique_ids = set(np.concatenate(id_tuple))\n",
    "    random_id = np.random.choice(range(len(unique_ids)), size=len(unique_ids), replace=False)\n",
    "    return {k: v for k, v in zip(list(unique_ids), random_id)}\n",
    "\n",
    "def repeat_data(weighted_data, weight_col=\"weight\"):\n",
    "    \"\"\" Repeat each row n times where n is described by the weight_col\"\"\"\n",
    "    return pd.DataFrame(weighted_data.values.repeat(weighted_data[weight_col], axis=0), \n",
    "                        columns=weighted_data.columns)\n",
    "\n",
    "def group_weight_pipe(df):\n",
    "    \"\"\" Creates a df with index of group ids and a column with msg weights + series with group sizes\"\"\"\n",
    "    group_sizes = calc_group_sizes(consent_df)\n",
    "    return pd.DataFrame(group_sizes).assign(weight = calculate_group_weights(group_sizes))[\"weight\"], group_sizes\n",
    "\n",
    "def add_group_weights(df):\n",
    "    \"\"\" calculates and joins the group weights to the original dataframe \"\"\"\n",
    "    group_weights, group_sizes = group_weight_pipe(df)\n",
    "    merged_msgs = pd.merge(df, group_weights, how=\"left\", left_on=\"to\", right_index=True)\n",
    "    merged_msgs[\"weight\"] = merged_msgs[\"weight\"].fillna(np.floor(np.log2(group_sizes.max())))\n",
    "    return merged_msgs\n",
    "\n",
    "def merge_group_members(df, weighted_group):\n",
    "    \"\"\" Joins the members of a group to the group_id, creating a much longer dataframe \"\"\"\n",
    "    groups = df.loc[df[\"rel_type\"] == \"group\", [\"from\", \"to\"]]\n",
    "    return pd.merge(weighted_group[weighted_group[\"rel_type\"] != \"group\"], groups, how=\"left\", on=\"to\")\n",
    "\n",
    "def clean_merged_group(group_merge):\n",
    "    \"\"\" Cleans up the merged group dataframe, renaming and dropping nans\"\"\"\n",
    "    return group_merge.assign(to_person=group_merge[\"from_y\"].combine_first(group_merge[\"to\"]))[[\"from_x\", \"to_person\", \"timestamp\", \"rel_type\", \"weight\"]] \\\n",
    "           .rename({\"from_x\": \"from\", \"to_person\":\"to\"}, axis=1) \\\n",
    "           .replace([np.inf, -np.inf], np.nan) \\\n",
    "           .dropna()\n",
    "\n",
    "def pathpy_pipeline(df):\n",
    "    \"\"\" Full pipeline for getting dataframe in Pathpy friendly format\"\"\"\n",
    "    weighted_group = add_group_weights(df)\n",
    "    merged_group = merge_group_members(df, weighted_group)\n",
    "    final_merge = clean_merged_group(merged_group)\n",
    "    repeated_data = repeat_data(final_merge)\n",
    "    return repeated_data[[\"from\", \"to\", \"timestamp\"]]\n",
    "\n",
    "\n",
    "def tidy_pipeline(df):\n",
    "    \"\"\" Full pipeline for tidyverse data \"\"\"\n",
    "    weighted_group = add_group_weights(df)\n",
    "    merged_group = merge_group_members(df, weighted_group)\n",
    "    final_merge = clean_merged_group(merged_group)\n",
    "    return final_merge[[\"from\", \"to\", \"timestamp\", \"weight\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 3,
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./data\")\n",
    "anonymize_folder(DATA_DIR)\n",
    "data_paths = list(DATA_DIR.glob(\"*.zip\"))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 4,
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_df = create_dropout_df(data_paths)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 5,
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [None for _ in range(len(data_paths))]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 11,
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
<<<<<<< HEAD
      "processing person 1 out of 29...\n",
      "Pending\n",
=======
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
      "processing person 2 out of 29...\n",
      "processing person 3 out of 29...\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "Pending\n",
      "processing person 4 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "processing person 5 out of 29...\n",
      "processing person 6 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "processing person 7 out of 29...\n",
<<<<<<< HEAD
=======
      "processing person 8 out of 29...\n",
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
      "Pending\n",
      "PendingGroup\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
<<<<<<< HEAD
      "processing person 8 out of 29...\n",
      "processing person 9 out of 29...\n",
      "processing person 10 out of 29...\n",
      "processing person 11 out of 29...\n",
      "PendingGroup\n",
      "processing person 12 out of 29...\n",
      "PendingGroup\n",
      "processing person 13 out of 29...\n",
      "Pending\n",
      "processing person 14 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "processing person 15 out of 29...\n",
      "PendingGroup\n",
      "Pending\n",
      "processing person 16 out of 29...\n",
      "Pending\n",
      "processing person 17 out of 29...\n",
      "processing person 18 out of 29...\n",
=======
      "processing person 9 out of 29...\n",
      "processing person 10 out of 29...\n",
      "processing person 11 out of 29...\n",
      "processing person 12 out of 29...\n",
      "PendingGroup\n",
      "processing person 13 out of 29...\n",
      "PendingGroup\n",
      "processing person 14 out of 29...\n",
      "Pending\n",
      "processing person 15 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "processing person 16 out of 29...\n",
      "PendingGroup\n",
      "Pending\n",
      "processing person 17 out of 29...\n",
      "Pending\n",
      "processing person 18 out of 29...\n",
      "processing person 19 out of 29...\n",
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
      "Pending\n",
      "processing person 19 out of 29...\n",
      "processing person 20 out of 29...\n",
      "processing person 21 out of 29...\n",
      "PendingGroup\n",
      "Pending\n",
      "processing person 22 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "processing person 23 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "processing person 24 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "processing person 25 out of 29...\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "PendingGroup\n",
      "processing person 26 out of 29...\n",
      "Pending\n",
      "processing person 27 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "processing person 28 out of 29...\n",
      "processing person 29 out of 29...\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "Pending\n",
      "all done!\n"
     ]
    }
   ],
   "source": [
    "for i, data_path in enumerate(data_paths):\n",
    "    if data_list[i] is None:\n",
    "        print(f\"processing person {i+1} out of {len(data_paths)}...\")\n",
    "        try:\n",
    "            data_list[i] = process_person(data_path)\n",
    "        except FileNotFoundError:\n",
    "            print(\"no file here\")\n",
    "            continue\n",
    "print(\"all done!\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.concat(data_list)"
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {hash_name(\"Lasse Hyldig Hansen\"): [hash_name(\"Lasse Hansen\")],\n",
    "                    hash_name(\"Pernille HÃ¸jlund Brams\"): [hash_name(\"Pernille Brams\")],\n",
    "                    hash_name(\"Tobias GrÃ¸nhÃ¸i Hansen\"): [hash_name(\"Tobias Hansen\")]}"
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_master = master_df.drop_duplicates(subset=['timestamp'])\n",
    "consent_df = filter_consent(unique_master)"
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.concat(data_list)\n",
    "unique_master = master_df.drop_duplicates()\n",
    "unique_master.to_csv(Path(\"../full_mess.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_master = pd.read_csv(Path(\"../full_mess.csv\"))\n",
    "unique_master = unique_master.replace(replacement_dict)\n",
    "fix_vero(unique_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog_df = remove_non_cogs(unique_master)\n",
    "cog_df.to_csv(Path(\"../cog_raw.csv\"), index=False)\n",
    "consent_df = filter_consent(cog_df, dropout_df[\"name\"])\n",
    "#consent_df.to_csv(\"raw_consensual.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhr\\Anaconda3\\envs\\cogmess\\lib\\site-packages\\pandas\\core\\frame.py:4166: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "random_replacement_dict = create_random_ids((consent_df[\"from\"], consent_df[\"to\"], dropout_df[\"name\"]))\n",
    "consent_df.replace(random_replacement_dict, inplace=True)\n",
    "dropout_df.replace(random_replacement_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "consent_df.to_csv(\"raw_consensual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consent_df = pd.read_csv(\"raw_consensual.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315,)\n",
      "(304,)\n",
      "(28,)\n"
     ]
    }
   ],
   "source": [
    "tidy_df = tidy_pipeline(consent_df)\n",
    "tidy_df.to_csv(\"tidy_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathpy_df = pathpy_pipeline(consent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogsci_participants = [\n",
    "    {\n",
    "      \"name\": \"Morten R. Gade\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Lasse Hyldig Hansen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"S\\u00c3\\u00b8ren Orm Hansen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Therese Ullerup\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Emma-Louise Alb\\u00c3\\u00a6k Schnedler\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Nikolaj Munch\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Clement Benjamin Breinholdt Peters\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Cassandra Rempel\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Helle Skj\\u00c3\\u00b8th S\\u00c3\\u00b8rensen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Daniel Elmstr\\u00c3\\u00b8m Christensen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Anders Hjulmand\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Julie Svinth Nielsen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Sarah Hedvig Dahl Nielsen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Gustav Metzsch\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Sophie Stenderup Korch\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Manon \\u00c3\\u0089lo\\u00c3\\u00afse Albeck Grandjean\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Pernille H\\u00c3\\u00b8jlund Brams\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Cecilie Stilling Pedersen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Alberte Aaskov Jakobsen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Nina Caroline H\\u00c3\\u00b8j Illum\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Kristian Severin Mengel-Niemann\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Kasper Michelsen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Kevan Vedrines\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Jesper Fischer\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Andrea Dioni Munksgaard\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Sarah Hvid Andersen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Esben Kran Christensen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Sigrid Agersnap Bom Nielsen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Nanna H\\u00c3\\u00b8gsholt\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Elisabet Vick\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Kristine Torp\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Martine Lind Jensen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Lina Elkj\\u00c3\\u00a6r Pedersen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Nicoline Schmidt\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Gustav Helms\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Verus Juhasz\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Andreas Hjorth Jeppesen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Thea Pedersen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Liv Toll\\u00c3\\u00a5nes\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Mia Jacobsen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Jonathan Hvithamar Rystr\\u00c3\\u00b8m\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Klara Kr\\u00c3\\u00b8yer Fomsgaard\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Maria Nissen Byg\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Astrid Kj\\u00c3\\u00a6r Olling\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Magnus Sonne Bom\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"David Fjendbo\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Alba Herrero\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Anna Hedvig M\\u00c3\\u00b8ller Daugaard\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Cecilie Vestergaard\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Gacilda Anne\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"L\\u00c3\\u00a6rke Br\\u00c3\\u00a6dder\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Tobias Gr\\u00c3\\u00b8nh\\u00c3\\u00b8i Hansen\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Julia J\\u00c3\\u00bcnger\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Miriam Diaz\"\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1           Lasse Hyldig Hansen\n",
       "16      Pernille HÃ¸jlund Brams\n",
       "17    Cecilie Stilling Pedersen\n",
       "46                 Alba Herrero\n",
       "51      Tobias GrÃ¸nhÃ¸i Hansen\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cogsci_name_list = [dic[\"name\"] for dic in cogsci_participants]\n",
    "cogsci_check = [is_a_cogsci(cogs, name) for name in cogsci_name_list]\n",
    "cogname_df = pd.DataFrame({\"name\": cogsci_name_list, \"hash_check\": cogsci_check})\n",
    "cogname_df.loc[~cogname_df[\"hash_check\"], \"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogs.add(hash_name(\"Cecilie Stilling Pedersen\"))\n",
    "cogs.add(hash_name(\"Alba Herrero\"))\n",
    "with open(Path(\"../cogsci19_2.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(cogs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Fix navne-svipsere (se cogname_df) (Rimelig done)\n",
    "- Få implementeret multiply logic (for vægtning) (DONE)\n",
    "- Få fikset hash-svupsere (identificer og fix)"
>>>>>>> 70f9387b42f4b9fc4f76a8bb2a483cf75b3f799e
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "consent_df.to_csv(\"all_messages.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}