{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_msg_files(zip_path, target_dir):\n",
    "    with ZipFile(zip_path, 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        all_files = zipObj.namelist()\n",
    "        for file in all_files:\n",
    "            if file.endswith(\".json\"):\n",
    "                zipObj.extract(file, target_dir)\n",
    "\n",
    "                \n",
    "def yield_msg_files(zip_path):\n",
    "    \"\"\" Creates generator for files in zipdir \"\"\"\n",
    "    with ZipFile(zip_path, 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        all_files = zipObj.namelist()\n",
    "        for file in all_files:\n",
    "            with zipObj.open(file, \"r\") as myfile:\n",
    "                yield json.loads(json.load(myfile))\n",
    "                    \n",
    "\n",
    "\n",
    "def read_zip_file(zip_path, file_name):\n",
    "    with ZipFile(zip_path, \"r\") as zipObj:\n",
    "        with zipObj.open(file_name, \"r\") as f:\n",
    "            return json.load(f)\n",
    "                    \n",
    "def read_json(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def read_convo(file):\n",
    "    \"\"\"reads conversation json file to dict \"\"\"\n",
    "    return json.loads(read_json(file))\n",
    "\n",
    "def hash_name(name):\n",
    "    \"\"\" simplified version (no salt) \"\"\"\n",
    "    return hashlib.sha1(name.encode()).hexdigest()\n",
    "\n",
    "def create_group_id(groupchat):\n",
    "    \"\"\"creates a group id based on participant names\"\"\"\n",
    "    participant_string = \"\".join(sorted(groupchat[\"participants\"]))\n",
    "    return hash_name(participant_string)\n",
    "\n",
    "def find_most_common(participant_list):\n",
    "    \"\"\"finds most common element in list \"\"\"\n",
    "    return Counter(participant_list).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_reactions(msg, rel_list):\n",
    "    \"\"\" Appends reaction to a reaction list (preprocessing step) \"\"\"\n",
    "    if \"reactions\" in msg.keys():\n",
    "        for reaction in msg[\"reactions\"]:\n",
    "            reaction_dict = {\"from\": reaction, \n",
    "                             \"to\": msg[\"sender_name\"], \n",
    "                             \"timestamp\": msg[\"timestamp_ms\"], \n",
    "                             \"rel_type\": \"reaction\"}\n",
    "            rel_list.append(reaction_dict)\n",
    "\n",
    "            \n",
    "            \n",
    "def create_member_edges(group_convo, group_id):\n",
    "    \"\"\" Create participant --> group relations for a conversation \"\"\"\n",
    "    return pd.DataFrame({\"from\": group_convo[\"participants\"], \n",
    "                          \"to\": group_id, \n",
    "                          \"timestamp\": np.nan, \n",
    "                          \"rel_type\": \"group\"})\n",
    "\n",
    "def process_group_messages(group_convo, group_id):\n",
    "    \"\"\" Create a nice dataframe with all the messages from group chat\"\"\"\n",
    "    group_msgs = pd.DataFrame(index=range(len(test_group[\"messages\"])), \n",
    "                              columns=[\"from\", \"to\", \"timestamp\", \"rel_type\"])\n",
    "    group_msgs = group_msgs.assign(to = group_id, rel_type = \"msg\")\n",
    "    rel_list = []\n",
    "    for i, msg in enumerate(test_group[\"messages\"]):\n",
    "        group_msgs.loc[i, \"from\"] = msg[\"sender_name\"]\n",
    "        group_msgs.loc[i, \"timestamp\"] = msg[\"timestamp_ms\"]\n",
    "        add_reactions(msg, rel_list)\n",
    "    return pd.concat([group_msgs, pd.DataFrame(rel_list)])\n",
    "\n",
    "def process_group_edges(group_convo):\n",
    "    \"\"\" Full pipeline for processing group chats \"\"\"\n",
    "    group_id = create_group_id(group_convo)\n",
    "    group_msgs = process_group_messages(group_convo, group_id)\n",
    "    group_members = create_member_edges(group_convo, group_id)\n",
    "    return pd.concat([group_msgs, group_members]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def process_msgs(convo):\n",
    "    \"\"\" Processes messages and returns a nice dataframe :)) \"\"\"\n",
    "    msgs = pd.DataFrame(index=range(len(test_chat[\"messages\"])), \n",
    "                        columns=[\"from\", \"to\", \"timestamp\", \"rel_type\"])\n",
    "    msgs = msgs.assign(rel_type = \"msg\")\n",
    "    rel_list\n",
    "    for i, msg in enumerate(test_chat[\"messages\"]):\n",
    "        if \"call_duration\" in msg.keys():\n",
    "            continue\n",
    "        msgs.loc[i, \"from\"] = msg[\"sender_name\"]\n",
    "        msgs.loc[i, \"to\"] = msg[\"receiver_name\"]\n",
    "        msgs.loc[i, \"timestamp\"] = msg[\"timestamp_ms\"]\n",
    "        add_reactions(msg, rel_list)\n",
    "    return pd.concat([msgs.dropna(subset=[\"from\"])\n",
    "                            , pd.DataFrame(rel_list)])\n",
    "\n",
    "\n",
    "def fix_dropout_dict(data_path):    \n",
    "    \"\"\"adds name to dropout dict as well as fixes key\"\"\"\n",
    "    file_generator = Path(data_path).glob(\"*.json\")\n",
    "    data_files = [file for file in file_generator if file.name != \"dropout.json\"]\n",
    "\n",
    "\n",
    "    participant_list = []\n",
    "    num_two_person = 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        for convo in yield_msg_files(data_path):\n",
    "            print(convo[\"thread_type\"])\n",
    "            is_two_person = convo[\"thread_type\"] == \"Regular\"\n",
    "            if is_two_person:\n",
    "                num_two_person += 1\n",
    "                participant_list.extend(convo[\"participants\"])      \n",
    "            if num_two_person == 2:\n",
    "                stop = True\n",
    "                break\n",
    "        \n",
    "    dropout_dict = read_zip_file(data_path, \"dropout.json\")\n",
    "    dropout_dict[\"still_cogsci\"] = dropout_dict.pop(\"is_dropout\")\n",
    "    dropout_dict[\"name\"] = find_most_common(participant_list)\n",
    "    return dropout_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data\")\n",
    "\n",
    "data_files = data_dir.glob(\"*.zip\")\n",
    "for file in data_files:\n",
    "    data_target = data_dir / f\"{file.name[:-4]}_unzipped\"\n",
    "    unzip_msg_files(file, data_target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jhr\\\\cogsci\\\\soccult'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = list(data_dir.glob(\"./*zip\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/all_the_data_0NKSA2CJ.zip')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_msg_files(zip_path, target_dir):\n",
    "    with ZipFile(zip_path, 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        all_files = zipObj.namelist()\n",
    "        for file in all_files:\n",
    "            if file.endswith(\".json\"):\n",
    "                zipObj.extract(file, target_dir)\n",
    "                \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegularGroup\n",
      "Regular\n",
      "Regular\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-0f07b75e3d3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdropout_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix_dropout_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-26024d011462>\u001b[0m in \u001b[0;36mfix_dropout_dict\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[0mdropout_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_zip_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dropout.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[0mdropout_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"still_cogsci\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"is_dropout\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0mdropout_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_most_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparticipant_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-26024d011462>\u001b[0m in \u001b[0;36mread_zip_file\u001b[1;34m(zip_path, file_name)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cogmess\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n\u001b[0m\u001b[0;32m    342\u001b[0m                             f'not {s.__class__.__name__}')\n\u001b[0;32m    343\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'surrogatepass'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not dict"
     ]
    }
   ],
   "source": [
    "dropout_dict = fix_dropout_dict(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'participants': ['12688c448b8135515026f4d932de436b37083fae', '23f06d0b724d5b5ff893a269281d1a79f0bcb6b2', '4b01401aca70648e91556d8666179b84ab1af68b'], 'messages': [{'sender_name': '4b01401aca70648e91556d8666179b84ab1af68b', 'timestamp_ms': 1577092657453}, {'sender_name': '23f06d0b724d5b5ff893a269281d1a79f0bcb6b2', 'timestamp_ms': 1577087169190}], 'thread_type': 'RegularGroup', 'subscriptions': []}\n"
     ]
    }
   ],
   "source": [
    "for testy in yield_msg_files(file):\n",
    "    print(testy)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipped_dirs = data_dir.glob(\"./*unzipped/\")\n",
    "dropout_dicts = [fix_dropout_dict(dat_dir) for dat_dir in unzipped_dirs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"participants\": [\"12688c448b8135515026f4d932de436b37083fae\", \"23f06d0b724d5b5ff893a269281d1a79f0bcb6b2\", \"4b01401aca70648e91556d8666179b84ab1af68b\"], \"messages\": [{\"sender_name\": \"4b01401aca70648e91556d8666179b84ab1af68b\", \"timestamp_ms\": 1577092657453}, {\"sender_name\": \"23f06d0b724d5b5ff893a269281d1a79f0bcb6b2\", \"timestamp_ms\": 1577087169190}], \"thread_type\": \"RegularGroup\", \"subscriptions\": []}\n"
     ]
    }
   ],
   "source": [
    "for convo in yield_msg_files(data_path):\n",
    "    testy = convo\n",
    "    print(testy)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo = {\"participants\": [\"12688c448b8135515026f4d932de436b37083fae\", \"23f06d0b724d5b5ff893a269281d1a79f0bcb6b2\", \"4b01401aca70648e91556d8666179b84ab1af68b\"], \"messages\": [{\"sender_name\": \"4b01401aca70648e91556d8666179b84ab1af68b\", \"timestamp_ms\": 1577092657453}, {\"sender_name\": \"23f06d0b724d5b5ff893a269281d1a79f0bcb6b2\", \"timestamp_ms\": 1577087169190}], \"thread_type\": \"RegularGroup\", \"subscriptions\": []}\n",
    "\n",
    "len(convo[\"participants\"]) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_groupchat = data_target / \"0TW8SR0DHVLTOC3HT5E58WKKWOJFIFX5.json\"\n",
    "test_chatname = data_target / \"9Q9QMR3VY9P1VHO8W5OO1JJUB239MY8C.json\"\n",
    "test_group = read_convo(test_groupchat)\n",
    "test_chat = read_convo(test_chatname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = pd.DataFrame(index=range(len(test_chat[\"messages\"])), \n",
    "                    columns=[\"from\", \"to\", \"timestamp\", \"rel_type\"])\n",
    "msgs = msgs.assign(rel_type = \"msg\")\n",
    "rel_list\n",
    "for i, msg in enumerate(test_chat[\"messages\"]):\n",
    "    if \"call_duration\" in msg.keys():\n",
    "        continue\n",
    "    msgs.loc[i, \"from\"] = msg[\"sender_name\"]\n",
    "    msgs.loc[i, \"to\"] = msg[\"receiver_name\"]\n",
    "    msgs.loc[i, \"timestamp\"] = msg[\"timestamp_ms\"]\n",
    "    add_reactions(msg, rel_list)\n",
    "final_msgs = pd.concat([msgs.dropna(subset=[\"from\"])\n",
    "                        , pd.DataFrame(rel_list)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
